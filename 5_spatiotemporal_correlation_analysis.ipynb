{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial and Temporal Correlation Analysis\n",
    "\n",
    "This tutorial demonstrates how to map the statistical relationship between two spatio-temporal variables.\n",
    "\n",
    "It also covers the following technical concepts:\n",
    "\n",
    "* using THREDDS and [OPeNDAP](https://en.wikipedia.org/wiki/OPeNDAP)\n",
    "* getting information on file dimensions\n",
    "* subsetting netCDF data from OPeNDAP\n",
    "* grid resampling\n",
    "* linear and rank correlation and their statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, we start with our imports\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "seaborn.set_style('dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using THREDDS and OPeNDAP\n",
    "\n",
    "One of the various benefits of netCDF format is that it can be made directly \n",
    "accessible over the internet (using a so-called OPeNDAP client) so that you \n",
    "can get a specific part of the data without having to download the entire file. \n",
    "Furthermore, you can connect to remotely accessible (so-called THREDDS) data \n",
    "catalogues, so that you can more easily view information on the data and get \n",
    "previews, before deciding whether to download. \n",
    "\n",
    "In this tutorial we will download some gridded data directly from the data \n",
    "catalogue that sits behind the [ausenv.online](http://ausenv.online) website. \n",
    "By way of introduction, make your way to the documentation on '_Downloading \n",
    "Data_' that you can find via the help (?) menu and button on the website. This \n",
    "should take you to [this web page](http://www.wenfo.org/wald/australias-environment/#Download).\n",
    "Under the header '_Direct Links_' you will see two links for \n",
    "each data set: \n",
    "\n",
    "* *Direct download*: If you were to click on one of these links the browser \n",
    "will ask you where you want to put the file, or it may even start downloading \n",
    "without warning. This is the traditional way of getting data. Because gridded \n",
    "data files can be very large, this is only a good idea if you really want to \n",
    "use all the data (for example, if you are doing a national-scale analysis). \n",
    "\n",
    "* *THREDDS*: If you click on one of these links your browser should open a \n",
    "new web page with the NCI logo up top and then a whole lot of fairly unintelligible \n",
    "text and links. Unfortunately, that is the default look of a THREDDS catalog. \n",
    "There are ways of making it look more informative, and indeed \n",
    "[the official NCI Catalogue](https://geonetwork.nci.org.au/geonetwork/srv/eng/catalog.search#/home)\n",
    "does that. However, we won't need it right now, as we can find \n",
    "the information we want by clicking through to the link that appears behind \n",
    "the word OPENDAP. You will now see an OPeNDAP Data Access Form bunch of information \n",
    "on this data file, including (1) the Data URL, which we will be using; (2) Global \n",
    "Attributes, containing some metadata; (3) Information on the variables contained; \n",
    "their dimensions, size, units, etc.\n",
    "\n",
    "With this information, we can write a few commands that directly gets the \n",
    "data we are interested in, without having to download the whole file first. \n",
    "To show that that is a major advantage, let's have a look at this \n",
    "([excerpted](http://www.wenfo.org/wald/australias-environment/#Download)):\n",
    "\n",
    "> - **Tree cover** (annual mapping): \n",
    "    [direct download](http://dapds00.nci.org.au/thredds/fileServer/ub8/au/treecover/250m/ANUWALD.TreeCover.AllYears.250m.nc)  (>500 MB) or \n",
    "    [THREDDS](http://dap.nci.org.au/thredds/remoteCatalogService?command=subset&catalog=http://dapds00.nci.org.au/thredds/catalog/ub8/au/treecover/250m/catalog.xml&dataset=ub8-au/treecover/250m/ANUWALD.TreeCover.AllYears.250m.nc)\n",
    "\n",
    "On the website you are warned that the entirely data file for Australia \n",
    "and all years is more than 500 MB, which is a waste of disk space if you are \n",
    "only interested in some part of Australia or a few years. Let's assume we are \n",
    "interested in tree cover for the same area near Canberra before and after the \n",
    "2003 fires, in 2002 and 2004, respectively. By following the THREDDS > OPENDAP \n",
    "link we can see the OPeNDAP Data Access Form and copy the Data URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_data_url = 'http://dapds00.nci.org.au/thredds/dodsC/ub8/au/treecover/250m/ANUWALD.TreeCover.AllYears.250m.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*At this point, we skip about a quarter of the Matlab tutorial, where you have to look up the order of the dimensions, calculate the pixel coordinates of the area you want to subset, and fiddle with a number of other tedious and error-prone steps.  Fortunately you are using Python, and the Xarray package does all of this for you.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_cover = xr.open_dataset(tree_data_url)\n",
    "tree_cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the shape of this data - it's more than ten thousand steps along each of the spatial dimensions!\n",
    "As a general rule of thumb, this will make it too big to load into memory at once.\n",
    "\n",
    "Let's check nbytes to see the total size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_cover.nbytes / 10 **9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we dodge another chunk of the Matlab tutorial, where simply finding the data in a file is tricky.  Xarray's named dimensions make this much more pleasant!  We also get the benefit of \"lazy loading\", meaning we can read the whole file and* then *select the area we want, because the data is not fetched until it is needed.  In Matlab, you have to manaully calculate which part of the data you need before opening the dataset!*\n",
    "\n",
    "Remember, to select all data inside a bounding box we select a `slice` from start to end, in coordinates.  Datetime coordinates [can be selected with partial strings](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#partial-string-indexing) in the form `YYYY-MM-DD`, and you can stop at any part.  In Xarray and in Pandas - which share their index and coordinate logic - a slice according to coordinates includes the endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in decimal degrees, taken from Tutorial 3\n",
    "lat_bounds = slice(-35.250, -35.625)\n",
    "lon_bounds = slice(148.750, 149.000)\n",
    "\n",
    "# Note: this means \"all times from the start of 2002 to the end of 2004\".\n",
    "# This includes 2003, but by coincidence there is no data for 2003!\n",
    "time_bounds = slice('2002', '2004')\n",
    "\n",
    "canberra_tree_cover = tree_cover.sel(\n",
    "    latitude=lat_bounds, longitude=lon_bounds, time=time_bounds)\n",
    "canberra_tree_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the size of our subset, so we know\n",
    "# whether it's a good idea to download it into memory\n",
    "canberra_tree_cover.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our subset is measured in KB, we'll download it now\n",
    "canberra_tree_cover.load()\n",
    "# And then draw some maps!\n",
    "canberra_tree_cover.AllYears.plot.imshow(col='time', cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for going back to this area and looking at forests was to make \n",
    "it easier for you to see if you got the correct part of the data cube. As a \n",
    "point of interest, if you compare the change from 2002 to 2004 above to the \n",
    "burn severity map you will see some similarities as well as differences. Mostly, \n",
    "this comes from the definition that is used here to map forests, as '_vegetation \n",
    "that has the potential to reach 2 metres height and more than 20% canopy cover_' \n",
    "(the definition used in the National Carbon Accounting System). The 'potential' \n",
    "bit has the interesting consequence that mapping of burnt forests needs to be \n",
    "adjusted back in time if there is evidence that the forest has regrown, because \n",
    "in retrospect it then evidently had the _potential_ to become forest. \n",
    "\n",
    "If you think this was confusing, you are not the only one - remember that this\n",
    "definition is designed to facilitate payments or penalties for carbon farming\n",
    "and land clearing, *not* as a biophysically meaningful summary.\n",
    "\n",
    "\n",
    "## Mapping statistical relationships between variables\n",
    "\n",
    "For a change of scenery, let's have a look at some different data, a different \n",
    "region, and different application. An exposed soil surface is vulnerable to \n",
    "wind and water erosion, and exposed soil can usually be considered in worse \n",
    "health than protected soil. Several government and semi-government bodies (e.g., \n",
    "catchment management authorities) have targets for soil surface cover protection, \n",
    "whether from living or dead vegetation (litter). As you can imagine, there is \n",
    "often a link between soil moisture and soil exposure, and this makes it harder \n",
    "to separate rainfall-related variations in soil exposure from long-term trends. \n",
    "Here, we will investigate this by:\n",
    "\n",
    "- Finding the correlation between soil moisture and soil protective cover.\n",
    "- Establishing a linear regression model to account for this relationship.\n",
    "- Calculate the residual change in soil cover after accounting for the influence of soil moisture. \n",
    "- Interpreting the result in terms of land management.\n",
    "\n",
    "To do that, we will use annual average soil moisture and soil exposure \n",
    "data from the Australia's Environment Explorer (http://ausenv.online). The soil \n",
    "exposure data are produced by CSIRO Land and Water based on MODIS remote sensing \n",
    "using the method published in \n",
    "[Guerschman et al. (2015)](http://www.sciencedirect.com/science/article/pii/S0034425715000395).\n",
    "The soil moisture data are modelled (based on rainfall and other ground \n",
    "and satellite observations) by ANU using a version of the AWRA-L model\n",
    "([Van Dijk, 2010](http://www.clw.csiro.au/publications/waterforahealthycountry/2010/wfhc-aus-water-resources-assessment-system.pdf))\n",
    "(You can find these links in the netCDF metadata as well)\n",
    "\n",
    "You will eventually need to decide on your region of interest. Here we will \n",
    "have a look at the greater Melbourne area, chosen because it has city, forests, \n",
    "pasture, cropland and water.  The bounds we will use are defined as:\n",
    "\n",
    "**Remember: you can do this for any area you like - just come back later and set different bounds.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No time bounds this time - we'll use the whole timeseries\n",
    "melb_lat_bounds = slice(-37.25, -38.15)\n",
    "melb_lon_bounds = slice(144.45, 145.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we already know the bounds we want, we can open, select, load the data, and reorder lat/lon axes in one go\n",
    "bare_soil = xr.open_dataset(\n",
    "    'http://dapds00.nci.org.au/thredds/dodsC/ub8/au/FractCov/BS/FractCover.V3_0_1.AnnualMeans.aust.005.BS.nc')\\\n",
    "    .sel(latitude=melb_lat_bounds, longitude=melb_lon_bounds).load()\\\n",
    "    .AnnualMeans.transpose('time', 'latitude', 'longitude')\n",
    "# Fix some names in the metadata - see what happens if you comment out these lines\n",
    "bare_soil.name = 'bare_soil_fraction'\n",
    "bare_soil.attrs = dict(short_name='bare_soil_fraction', long_name='Annual mean fraction of bare soil')\n",
    "# It was stored as an integer, so divide by 100 to get fraction from percent\n",
    "bare_soil /= 100\n",
    "# And finally display the array\n",
    "bare_soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bare_soil.isel(time=-1).plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we do the same thing for soil moisture ()\n",
    "soil_moisture = xr.open_dataset(\n",
    "    'http://dapds00.nci.org.au/thredds/dodsC/ub8/au/OzWALD/annual/OzWALD.annual.Ssoil.AnnualMeans.nc')\\\n",
    "    .sel(latitude=melb_lat_bounds, longitude=melb_lon_bounds).load()\\\n",
    "    .AnnualMeans.transpose('time', 'latitude', 'longitude')\n",
    "soil_moisture.name = 'soil_moisture_mm'\n",
    "soil_moisture.attrs['short_name'] = soil_moisture.name\n",
    "soil_moisture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_moisture.isel(time=-1).plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you may notice that the soil moisture data look a lot more 'blocky' \n",
    "(pixellated would be a more technical term) then the soil exposure data, meaning\n",
    "that the spatial resolution of the former is several times lower than that of the latter.\n",
    "This is a common occurrence when working with two gridded data sets. \n",
    "\n",
    "Because most analyses require arrays of the same shape (size and dimension), you will need\n",
    "to resample in some way, choosing a target resolution and interpolation method.\n",
    "A common choice is to resample to the shape of the predictand - in our case exposed \n",
    "soil fraction - or to choose the finest grid that is convenient to work with\n",
    "(ie coarsening very large data).\n",
    "\n",
    "[Iterpolating](https://en.wikipedia.org/wiki/Interpolation), or \"zooming\" an image\n",
    "is a common operation - and a common source of misinterpreted data.  The two main\n",
    "approaches are to fit a polynomial function to all the data points in a sequence,\n",
    "or to fit a function *between* each point in such a way that they fit smoothly \n",
    "togther (called a \"spline\" - this is also the fundamental unit of vector data).\n",
    "Unless a deep understanding of your specific data calls for something exotic,\n",
    "and there are some very exotic methods that only work for specifc kinds of data,\n",
    "spline interpolation is a very good choice.\n",
    "\n",
    "The most important parameter is the *order* of the function(s) to fit.\n",
    "\n",
    "- order=0 is nearest-neighbor; the new grid is still blocky but each grid consists of multiple pixels\n",
    "- order=1 is linear; a poor choice for regular grids (interpolation works for irregular points too)\n",
    "- order=3 is cubic; this is usually the best choice for images\n",
    "\n",
    "See [wikipedia](https://en.wikipedia.org/wiki/Interpolation#In_higher_dimensions)\n",
    "for an example of each type.  Of course it is also possible to use higher-order splines,\n",
    "but they are only really useful on higher-dimensional (ie more than two) data.\n",
    "\n",
    "We will use [`scipy.ndimage.zoom`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.zoom.html)\n",
    "on each time-step of the soil moisture array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use scipy.image.ndzoom to upscale soil moisture\n",
    "from scipy.ndimage import zoom as ndzoom\n",
    "help(ndzoom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the tricky bit: we will create a copy of our `bare_soil` data, replace the metadata, and finally fill it with the interpolated `soil_moisture` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "fine_moisture = deepcopy(bare_soil)\n",
    "fine_moisture.attrs = soil_moisture.attrs\n",
    "fine_moisture.name = soil_moisture.name\n",
    "fine_moisture['time'] = soil_moisture.time\n",
    "\n",
    "SPLINE_ORDER = 3  # Try some other values 0 to 5 for SPLINE_ORDER to see what happens\n",
    "ZOOM_FACTOR = (len(bare_soil.latitude) / len(soil_moisture.latitude),\n",
    "               len(bare_soil.longitude) / len(soil_moisture.longitude))\n",
    "\n",
    "# It's also possible to zoom a 3D array by setting a factor of 1 for the time\n",
    "# dimension, but this way we preserve the timesteps correctly.\n",
    "for timestamp in fine_moisture.time:\n",
    "    # Start by selecting the timestamp\n",
    "    data = soil_moisture.sel(time=timestamp)\n",
    "    # Then zoom to the desired scale, filling nodata values with zero so we can zoom\n",
    "    output = ndzoom(np.nan_to_num(data), zoom=ZOOM_FACTOR, order=SPLINE_ORDER)\n",
    "    # Assign output to the contents of the fine_moisture array\n",
    "    fine = fine_moisture.sel(time=timestamp)\n",
    "    fine[:] = output\n",
    "    \n",
    "    # Make sure the minimum is zero, so it remains physically plausible\n",
    "    fine.values[fine.values < 0] = 0\n",
    "    # Last, we'll copy both sets of NaN values so that we don't cause spurious correlations\n",
    "    # Try commenting each of these out to see how the map changes!\n",
    "    fine.values[np.isnan(bare_soil.sel(time=timestamp).values)] = np.nan  # from the high-res data\n",
    "    fine.values[ndzoom(np.isnan(data), zoom=ZOOM_FACTOR, order=0)] = np.nan  # from low-res, with nearest (blocky) zooming\n",
    "\n",
    "fine_moisture.isel(time=-1).plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice the data look rather different, which is a logical consequence \n",
    "of interpolation. However, you will also notice the effect of NaN values, which \n",
    "stuff up interpolation, and may start to appreciate that these missing data \n",
    "can be quite a pain.  (try changing how they are handled, as commented above)\n",
    "\n",
    "Working out how to handle missing data when resampling can be tricky - for example,\n",
    "maybe we should have filled them with the closest value to avoid skewing coastal\n",
    "areas towards zero.  On the other hand, that would have skewed them a little high!\n",
    "There is no universial rule for this - you just need good judgement, and to document\n",
    "your methods so others can review and replicate your work.\n",
    "\n",
    "## Correlation analysis\n",
    "\n",
    "Now we have been able to load in the data and have made them of identical \n",
    "resolution, we can start to look at the actual correlation analysis. \n",
    "\n",
    "Recall once again [Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) - \n",
    "before we calculate correlations, it is a good habit to draw some plots.\n",
    "\n",
    "Let's start by picking a grid cell, and plotting the timeseries for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a few different values to see if the relationship holds\n",
    "_lat, _lon = 100, 100\n",
    "\n",
    "# To plot two lines on the same axes, we have to explicitly create and use a set of axes \n",
    "# For the second, `ax.twinx()` creates a clone of the axes with a shared x and independent y.\n",
    "fig, ax = plt.subplots()\n",
    "fine_moisture.isel(latitude=_lat, longitude=_lon).plot(ax=ax)\n",
    "bare_soil.isel(latitude=_lat, longitude=_lon).plot(ax=ax.twinx(), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we do see the expected correlation - less moisture tends to mean more exposure.\n",
    "\n",
    "Next, we'll prepare our data for the correlation analysis by converting it into a pair of one-dimensional Pandas series.  This allows us to do efficient pairwise comparisons and calculations.  Pandas data does not have coordinates, but does have an index - so we can still be confident that corresponding data in each series will be matched up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe to hold our columns of data\n",
    "df = pd.DataFrame()\n",
    "# Convert each data array into a series, and add it to the dataframe\n",
    "for data in [bare_soil, fine_moisture]:\n",
    "    df[data.name] = data.to_series()\n",
    "# Discard any rows with missing values - I would usually keep them,\n",
    "# but we can't correlate anything with missing data\n",
    "df = df.dropna()\n",
    "\n",
    "# And examine the first five rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the right, you can see our two data columns.  On the left, we have our index - \n",
    "which consists of three subcolumns.  This is called a [MultiIndex](http://pandas.pydata.org/pandas-docs/stable/advanced.html);\n",
    "it works in the same way as a single-column index but allows us to group data by\n",
    "or convert back to higher dimensions.\n",
    "\n",
    "Xarray is fantastic for multidimensional gridded data, but Pandas is the standard\n",
    "tool for data analysis in Python.  Now that we have a Pandas dataframe, we can use\n",
    "all the general-purpose plotting and statistical tools too.\n",
    "\n",
    "First, a simple scatter plot.  Scatter plots are unreadable once you have more than\n",
    "a few thousand points, and running `df.size` will show that we have more than half a million rows of data.\n",
    "We'll therefore take a random sample - meaning a differnt plot every time you run the cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try sample numbers between 1 and 100,000; or even delete \".sample()\"\n",
    "df.sample(1000).plot.scatter(x='soil_moisture_mm', y='bare_soil_fraction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn, for statistical visualisation\n",
    "\n",
    "We've imported [Seaborn](http://seaborn.pydata.org) into our notebook each week, \n",
    "and appreciated the nicer colour schemes - but now it's time to see where it really shines.\n",
    "\n",
    "Matplotlib makes any plot possible; Pandas makes easy plots easy; and Seaborn is designed \n",
    "to make advanced statistical plots easy too - Xarray's support for drawing maps is\n",
    "inspired by Seaborn, in fact!\n",
    "\n",
    "For example, Seaborn makes it very easy to plot the joint distribution of two values with `jointplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.jointplot(\n",
    "    x='soil_moisture_mm',\n",
    "    y='bare_soil_fraction',\n",
    "    data=df.sample(10000),\n",
    "    # There are several ways to represent a join distribution.\n",
    "    # Try un-commenting one kind at a time!\n",
    "    #kind='hex', joint_kws=dict(gridsize=30),\n",
    "    kind='kde', cmap='magma_r', n_levels=200,\n",
    "    #kind='scatter',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `kde`, or '[kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation)', shows an estimate of the probabilities underlying our random sample.  The `hex` visualisation is an alternative to `scatter`, but less vulnerable to saturation where there are many similar values.\n",
    "\n",
    "Seaborn has even calculated the correlation coefficient (pearsonr ~= 0.35) and p-value of this corrlation (very strong).  However while we can be highly confident that an overall correlation exists, soil moisture only explains R-squared (10 to 15 percent) of the variation in exposed soil fraction across the whole area.\n",
    "\n",
    "If we had more columns in our dataframe, it would be equally easy to look at pairwise correlation and distributions using [`pairplot`](http://seaborn.pydata.org/generated/seaborn.pairplot.html).\n",
    "\n",
    "\n",
    "### Linear correlation\n",
    "\n",
    "In the last part of this notebook, we will use the `pearsonr` and `spearmanr` functions from SciPy\n",
    "to calculate the correlation between soil moisture and exposed soil in the timeseries for each pixel.\n",
    "\n",
    "The linear (or Pearson, or parametric) correlation coefficient is the most \n",
    "commonly measure the strength of the relationship between two variables. It \n",
    "is particularly well suited if both variables are close to normally distributed \n",
    "and a linear relationship can be assumed.  If the relationship seems non-linear,\n",
    "then it would be better to do one of two things: \n",
    "\n",
    "- Calculate the rank (or Spearman, or non-parametric) correlation coefficient. \n",
    "  To calculate the rank correlation, you replace each _x-_value by the rank (or \n",
    "  index) it would have if all _x_-values are sorted in ascending order, then do \n",
    "  the same for _y_, and then calculate the normal ('linear') correlation between\n",
    "  the resulting rank arrays.\n",
    "\n",
    "- Transform either or both variables to make the relationship closer to linear, \n",
    "  for example taking the logarithm or square root of _x_ or _y_, or both.\n",
    "\n",
    "In any event, it is always a good idea to always check whether the rank \n",
    "correlation is very different from the linear correlation coefficient. If the \n",
    "two approaches produce _R_- and _p_-values, that lead to similar conclusions, \n",
    "then that strengthens your analysis. However, if they produce different results, \n",
    "then that does not automatically mean that the relationship is not linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress, pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linregress(df.soil_moisture_mm, df.bare_soil_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(df.soil_moisture_mm, df.bare_soil_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(df.soil_moisture_mm, df.bare_soil_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the ranked _R_-value is actually quite different from \n",
    "the linear _R_-value, given that we are working with half a million rows.\n",
    "Looking at the plots above, the most likely reason is that the soil exposure \n",
    "data are not really (approximately) normally-distributed. Most values are between \n",
    "2 to 6 %, but while are some higher values, there are no lower values - logical, \n",
    "since soil exposure cannot be less than zero.\n",
    "\n",
    "Here, you could argue whether the data should be transformed (perhaps using _log(y)_ ).\n",
    "Instead, let's just calculate both types of correlation, and interpret them together.\n",
    "\n",
    "In the next two cells, we will calculate the correlations in each and every cell, and make\n",
    "a list of results which we then convert to an array.  *This is a very slow way to calculate anything*,\n",
    "but it works even for functions that are difficult to express as operations along some \n",
    "axis of the array - so I'll use if for correlations, but not for mean, min, max, sum and so on.\n",
    "Expect them to take two or three minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by setting up a new dataset, with empty arrays along latitude and longitude\n",
    "dims = ('latitude', 'longitude')\n",
    "coords = {d: bare_soil[d] for d in dims}\n",
    "correlation_data = {\n",
    "    name: xr.DataArray(data=np.ndarray([len(bare_soil[d]) for d in dims]),\n",
    "                       name=name, dims=dims)\n",
    "    for name in 'pearson_r pearson_p spearman_r spearman_p'.split()\n",
    "}\n",
    "corr = xr.Dataset(data_vars=correlation_data, coords=coords)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# By looping, we make a list of lists of correlations\n",
    "latout = []\n",
    "for lat in fine_moisture.latitude:\n",
    "    lonout = []\n",
    "    latout.append(lonout)\n",
    "    for lon in fine_moisture.longitude:\n",
    "        val = pearsonr(\n",
    "            fine_moisture.sel(latitude=lat, longitude=lon),\n",
    "            bare_soil.sel(latitude=lat, longitude=lon)\n",
    "        )\n",
    "        try:\n",
    "            # Spearman's R can fail for some values\n",
    "            val += spearmanr(\n",
    "                fine_moisture.sel(latitude=lat, longitude=lon),\n",
    "                bare_soil.sel(latitude=lat, longitude=lon)\n",
    "            )\n",
    "        except ValueError:\n",
    "            val += (np.nan, np.nan)\n",
    "        lonout.append(val)\n",
    "# Then we convert the lists to an array\n",
    "arr = np.array(latout)\n",
    "# And finally insert the pieces into our correlation dataset\n",
    "corr.pearson_r[:] = arr[..., 0]\n",
    "corr.pearson_p[:] = arr[..., 1]\n",
    "corr.spearman_r[:] = arr[..., 2]\n",
    "corr.spearman_p[:] = arr[..., 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset built, we can explore it as usual - for example, \n",
    "by plotting the r-values where significance exceeds some threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNIFICANT = 0.05  # Choose your own!\n",
    "corr.pearson_r.where(corr.pearson_p < SIGNIFICANT).plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.spearman_r.where(corr.spearman_p < SIGNIFICANT).plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have two nice maps showing where there are significant (at your choice of `p`, eg `<0.05`) \n",
    "relationships between mean annual soil moisture and soil exposure.\n",
    "\n",
    "* The two maps agree fairly well, which helps to make your interpretation more robust.\n",
    "* The obvious next step would be interpretation: why do some areas show a \n",
    "  strong relationship, whereas others do not, and indeed a few small areas (the \n",
    "  red spots) suggest a somewhat counterintuitive increase in soil exposure with \n",
    "  increased soil moisture? \n",
    "* A sensible way to approach this would be to export the data behind these maps as netcdf\n",
    "  (using `corr.to_netcdf('your_filename.nc')`), \n",
    "  import them in ArcGIS, and overlay them semi-transparently onto higher resolution \n",
    "  imagery, so that you might develop some hypotheses. For example, you might notice \n",
    "  that urbanised surfaces and forests mostly show less strong relations than do \n",
    "  grasslands, and hypothesise that grass cover disappears more quickly in response \n",
    "  to soil dryness. \n",
    "* Subsequently, you may be able to test those hypotheses in a more rigorous \n",
    "  way, perhaps by calculating the distribution of trends by land cover types, \n",
    "  using a grid-based land cover map.\n",
    "\n",
    "\n",
    "## Summary and Research Ideas\n",
    "\n",
    "This Tutorial focused on mapping the correlation between two spatiotemporal \n",
    "variables. This sort of analysis can provide important insights into the possible \n",
    "causes of observed phenomena (although correlation by itself should never be \n",
    "mistaken for causation). In this case, we demonstrated that in part of the Melbourne \n",
    "region, climate conditions (through soil moisture) are a strong influence on \n",
    "soil protective cover, and may dominate any effects of land management.\n",
    "\n",
    "You could easily adapt the code shown here to do a similar type of correlation \n",
    "analysis but for a different pair of environmental variables, and looking at \n",
    "a different geographical region. You can also combine the trend analysis shown \n",
    "in the previous Tutorial with a correlation analysis. For example, after mapping \n",
    "trends in one environmental variable (e.g., in vegetation, tree cover, soil \n",
    "exposure, carbon uptake) you may hypothesise that those are probably due to \n",
    "another driving variable (e.g. rainfall, soil moisture, fire occurrence), and \n",
    "test that by mapping correlation.\n",
    "\n",
    "Via [Australia's Environment Explorer](http://ausenv.online) \n",
    "you can find several environmental variables that are available in NetCDF-formatted \n",
    "data cubes. As suggested in the previous tutorial, have a look at the website, \n",
    "and see if there is anything that raises a question or research idea in your \n",
    "mind. You can also read the accompanying annual environment report for inspiration. \n",
    "\n",
    "Of course, there is a wide variety of other spatiotemporal data available \n",
    "online from other sources, and we can often help you find and access them if \n",
    "needed. If you have a research idea, by all means, discuss it with one of us.\n",
    "\n",
    "The next and final tutorial will look at combining vector polygon data \n",
    "and gridded data to calculate regional statistics, another common way of processing \n",
    "and summarising gridded data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:remote-sensing]",
   "language": "python",
   "name": "conda-env-remote-sensing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
